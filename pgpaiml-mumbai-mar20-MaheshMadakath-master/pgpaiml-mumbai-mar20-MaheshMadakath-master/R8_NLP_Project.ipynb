{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ob2nfNJ_3Q5"
   },
   "source": [
    "### R8 AIML NLP Project \n",
    "### Project1 - Multi label classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyGmMBgRACew"
   },
   "source": [
    "#### TASK1: PROJECT OBJECTIVE: The need is to build a NLP classifier which can use input text parameters to determine the label/s of of the blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbiogBF7ANKX"
   },
   "source": [
    "1. Import and analyse the data set.\n",
    "2. Perform data pre-processing on the data:\n",
    "    * Data cleansing by removing unwanted characters, spaces, stop words etc. Convert text to lowercase.\n",
    "    * Target/label merger and transformation\n",
    "    * Train and test split\n",
    "    * Vectorisation, etc.\n",
    "3. Design, train, tune and test the best text classifier.\n",
    "4. Display and explain detail the classification report\n",
    "5. Print the true vs predicted labels for any 5 entries from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "id": "7MWwNtfgDaZW"
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # Data Visualization\n",
    "import pandas as pd # Data processing\n",
    "import seaborn as sns # Data Visualization\n",
    "import numpy as np # Linear Algebra\n",
    "import tensorflow as tf #Tensor Flow\n",
    "from sklearn import preprocessing #preprocessing libraries from sklearn\n",
    "import re # regular expression\n",
    "import nltk #Import natural language toolkit to work with human languade\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB #Naive Bayes classifier for our case of multinomial model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer  #Sklearn text feature extraction and evaluation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV #Sklear train test split and grid search for hyper parameter tuning\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "740eEpZgDbXR",
    "outputId": "ca4d6b50-5971-45d9-89a7-0686c1e3f983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "A74N17u3Dv-j"
   },
   "outputs": [],
   "source": [
    "os.chdir('/gdrive/MyDrive/Colab Notebooks/Projects/NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h18rGH-6D2IG",
    "outputId": "a84ecbdf-fd61-4e62-e644-d21aa7f0e048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 781790\n",
      "-rw------- 1 root root 800419647 Apr  6 19:55 'Dataset - blogtext.csv'\n",
      "drwx------ 7 root root      4096 Apr  6 21:00  \u001b[0m\u001b[01;34mRajeevSiripynb\u001b[0m/\n",
      "-rw------- 1 root root      6520 Apr 11 09:18  GLBot.json\n",
      "-rw------- 1 root root         0 Apr 11 10:26  Corpus.csv\n",
      "-rw------- 1 root root    103909 Apr 11 13:59  model.tflearn.meta\n",
      "-rw------- 1 root root       890 Apr 11 13:59  model.tflearn.index\n",
      "-rw------- 1 root root     16257 Apr 11 13:59  model.tflearn.data-00000-of-00001\n",
      "-rw------- 1 root root       175 Apr 11 13:59  checkpoint\n"
     ]
    }
   ],
   "source": [
    "ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZHcv-AMGK-J"
   },
   "source": [
    "#### Step1: Import and analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "WdpMWVJCD5Ww"
   },
   "outputs": [],
   "source": [
    "BlogDf= pd.read_csv('Dataset - blogtext.csv') #import the data in to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRyIg0RXGTEb",
    "outputId": "a27b3718-f268-44e8-f5f8-f0bdde99f802"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 418,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlogDf.shape # Checking the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIKv9zRNGgn3",
    "outputId": "52f01270-2b29-4a3f-a5b6-600b12a7c3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681284 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      681284 non-null  int64 \n",
      " 1   gender  681284 non-null  object\n",
      " 2   age     681284 non-null  int64 \n",
      " 3   topic   681284 non-null  object\n",
      " 4   sign    681284 non-null  object\n",
      " 5   date    681284 non-null  object\n",
      " 6   text    681284 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "source": [
    "BlogDf.info() # Check for data type and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7kKheq30Fh_c",
    "outputId": "31abed2a-f26f-4ac7-9b2b-2b048a2b25c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51176</th>\n",
       "      <td>3389825</td>\n",
       "      <td>female</td>\n",
       "      <td>36</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>20,May,2004</td>\n",
       "      <td>Ok..I have to admit'I HAVENT REALLY DONE ANY WORK TODAY'  I have spent the majority of my time telling friends about blogger or responding to notes on www.blackplanet.com  Ok ok..I will do some work b4 I leave at 5pm- I promise..after I check my notes again on BP.  I know I know.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185005</th>\n",
       "      <td>3971148</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Leo</td>\n",
       "      <td>26,July,2004</td>\n",
       "      <td>So, I'm running out of pants to wear...I tried on an old pair yesterday that I forgot I even owned, and it took everything I had to button the button fly.&amp;nbsp; Oof-da.&amp;nbsp; The buttons could have been deadly weapons had they burst off and went airborne.&amp;nbsp; Somebody, and I'm not naming names, needs to slow down with the Ben and Jerry's.&amp;nbsp;     &amp;nbsp;  When I worked at Amoco, I was constantly moving around, doing things, walking everywhere.&amp;nbsp; Now that I work at the hospital, I sit in a chair mostly.&amp;nbsp; The pants I wore working at Amoco no longer fit me after a year and a half at the hospital.&amp;nbsp; Maybe there's something to be said for being a cashier.&amp;nbsp;     &amp;nbsp;   &amp;nbsp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30648</th>\n",
       "      <td>4252371</td>\n",
       "      <td>male</td>\n",
       "      <td>38</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Libra</td>\n",
       "      <td>17,August,2004</td>\n",
       "      <td>I just had a lovely memory of a most beautiful town square in India. Rishikesh to be precise. The town is most famous in the West for the visits The Beatles made to it in the sixties, but even in India it is well known. The town sits in the foothills of the Himalaya, straddling the newly born River Ganges. 25kms or so down river is Haridwar, one of the sites of the quadrennial religious gathering known as the  urlLink Kumbha Mela ; the largest gathering of humanity on the planet. As both Haridwar and Rishikesh are holy towns, meat is forbidden. They are strictly vegetarian towns.  Rishikesh itself is split into two parts, the main town, which is a very hectic, typically Indian town, full of blaring horns and holy cows. But on the far side of the river there is a part of the town where cars cannot come, populated by ashrams and the occasional guest house. To get there, one must take a boat or cross  urlLink a lovely footbridge  (which was built by the British during WWII). On the peak of the hill on that quiet side of town, there is a very serene, and seemingly very ancient, town square, which I stumbled across late one afternoon in 1993.  The afternoon light was lovely, and there were many sahdus (Indian holy men) sitting around. One sahdu was standing, but leaning on a swing hanging from a large tree. 'He's been standing for nearly 10 years', one of the sahdus later told me proudly. 'He sleeps standing, he eats standing, he even shits standing'. Wow, I thought. Impressive.  I smoked a chillum with a few of the sahdus as the light went from yellow to gold and the afternoon shone in its final glory. Whether it was the surroundings, the light, or just the hash, I will never be sure (most likely it was a combination of all three), but I felt a connection to the human social condition that afternoon that I have never felt since. The symbolism of the town square was intensely powerful - particularly without the modern irritations of traffic, noise or advertising. People came and went about their business, as they purchased vegetables for the evening meal, or just sat and talked to their neighbours at the end of the day. I was very envious of the standing sahdu. It was certainly a nice place to hang around (groan..) for a decade. Not that I envied him his feat of religious asceticism.  Just the view.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572235</th>\n",
       "      <td>3745396</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Sports-Recreation</td>\n",
       "      <td>Libra</td>\n",
       "      <td>06,July,2004</td>\n",
       "      <td>Back by popular demand is the third and final installment of Wisconsin Dells saga. It seems that one of my readers was not satisfied with the way I trailed off about the girls I met in the Dells. While I had more important things to do then meet with girls, I did spend a hefty amount of time with them.   The three girl's phone numbers I had previously announced I had are still in safe-keeping. I plan on calling 0 out of 3 of these girls and I will have nothing to do with any of them. The perfect girl in the pool, however, is a different story. There were two different pools in area that the guys and I swam in. Hence, there were two perfect girls; one in each pool. I apologize to the reader who had expected the number of pools to be at least ten so that there could be ten perfect girls in the world.   Anyways, while I'm on the subject, I did get to spend some time with one of the more attractive of the girls I met. She was a stunning dirty blonde with eyes that could make your legs weaken, lips that could make a grown man cry, and a smile that would melt the heart of any male. She had a cool demeanor and a strong aura of a rebel when I was near her. As much as I tried to get close to her, her independent aura made me back down like a kitten from a pit bull, afraid that she would reject me and leave. Luckily she did neither and we talked for what seemed like five minutes but in all actuality was more like two hours. I actually had the courage to ask her if she wanted to kiss...  'No.' she said. My heart sank. I was rejected. Well, it was worth a shot I had told myself. But then she added something that made my heart shoot back into my rib-cage. 'I can't. My mom is right behind the big dolphin.' Oh, thank God for the big dolphin. I had been saved by a six foot long plastic dolphin. I wanted nothing more than to hold her in my arms at the very least before I had to go, but it was cut short because I had to return to my hotel room for curfew.   Who knew that on one fateful night during a Wisconsin trip, I would meet the perfect girl. Someone to talk to, laugh with, and just have a good time with, without worrying about what anyone would say or do about it. I wish I could say I plan on meeting this girl again, but I don't. I do not see how that could be possible at all being in the great state of Indiana while my angel is in the equally as great, but very far away state of Minnesota.  The great thing is, I think Mariah felt the same about me. That's right, Mariah. Mariah Robinson. I leave you to ponder your own love story, reader. As I stroll off to bed wiping a single tear from my eye wondering myself, what could have been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228627</th>\n",
       "      <td>3297995</td>\n",
       "      <td>male</td>\n",
       "      <td>48</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>08,June,2001</td>\n",
       "      <td>You may recall that I had had pretty darn good vision after my recent cataract surgery, and I was (exceedingly) happy.  But my vision gradually degraded, as 50% of all cataract surgeries do, due to a clouding of the membrane that separates the lens from the back of the eye.  It was a gradual, but in the end, quite noticeable decline in acuity.  Well, I just got back from a quick outpatient procedure to correct this, and it couldn't have been more trouble-free.  To destroy the errant membrane, they shine multiple laser picobursts into your eye (about 20 to 30 of them), each one of which causes a pressure wave that punches a hole through the membrane.  Actually, a great metaphor that the surgeon used is that you should imagine a tightly-stretched film of Saran Wrap.   If you take a pin and pop a hole in the film, you'll get far more destruction than just a little hole, because the film is stretched so tightly.  So, I sat down, and he aimed his laser this way and that, popping the film with his laser bursts, until the membrane was essentially destroyed.  It was slightly startling, because I could FEEL the shock wave in the eye (like throwing a stone into a calm pond).  It took about three to five minutes - I didn't have to change out of my street clothes, even.  Many have asked, 'Don't you need all of your various body parts? Wasn't that part there for some reason?'  Well, it does more or less hold your lens in place, but the artificial lens that they put in during the cataract operation is sufficiently anchored down after 6 to 8 weeks that they can proceed with the destruction of the membrane.  And the results?  Amazing.  Using my evil eye, I after the operation, I could read the little badges on the cars with the brand name spelled out ('Mercury') from a distance of several car lengths.  And, for actual before-and-after comparison, I had put a large book (The Way To Cook, Julia Child, hardcover) against the piano and had seen how many paces away from the book I had to be before I couldn't read its title any more.  All tests were made with the evil eye open, and the excellent eye shut. Before the operation, WITH my glasses, I could just read the book title from about 8 feet away.  After the operation, WITHOUT my glasses, I could read the book from 10 feet away, and WITH my glasses, I could still read it from 23 feet away.  I might have been able to do a little better, but I ran out of living room.  So, almost a three-fold improvement in acuity, like going from 20/90 to 20/30.   Naturally, I'm thrilled.  (Even as I'm writing this just now, I can't help being tickled at how sharp everything looks).  I'm pretty confident now that even should my excellent eye suddenly have some horrible mishap, I would still easily qualify to drive with just my evil eye, especially if they would consent to clearing the freeways every morning and evening for my convenience.  And it's clearly good enough for me to be able to depend on it for reading and working, even should the other one fail.  The evil eye still has the dread macular pucker (the scar tissue on the retina, which makes things a bit wavy), and I'm still taking eye drops every day to prevent the retina from swelling, and my eye still LOOKS a little evil, but from the inside looking out, I'm deeply satisfied.  The retinal surgeon has no intention of operating on the macular pucker any time soon, so hopefully you won't hear from me about my dopey eye for a long, long time.  -Tom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text\n",
       "51176   3389825  ...         Ok..I have to admit'I HAVENT REALLY DONE ANY WORK TODAY'  I have spent the majority of my time telling friends about blogger or responding to notes on www.blackplanet.com  Ok ok..I will do some work b4 I leave at 5pm- I promise..after I check my notes again on BP.  I know I know.....                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "185005  3971148  ...              So, I'm running out of pants to wear...I tried on an old pair yesterday that I forgot I even owned, and it took everything I had to button the button fly.&nbsp; Oof-da.&nbsp; The buttons could have been deadly weapons had they burst off and went airborne.&nbsp; Somebody, and I'm not naming names, needs to slow down with the Ben and Jerry's.&nbsp;     &nbsp;  When I worked at Amoco, I was constantly moving around, doing things, walking everywhere.&nbsp; Now that I work at the hospital, I sit in a chair mostly.&nbsp; The pants I wore working at Amoco no longer fit me after a year and a half at the hospital.&nbsp; Maybe there's something to be said for being a cashier.&nbsp;     &nbsp;   &nbsp;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "30648   4252371  ...         I just had a lovely memory of a most beautiful town square in India. Rishikesh to be precise. The town is most famous in the West for the visits The Beatles made to it in the sixties, but even in India it is well known. The town sits in the foothills of the Himalaya, straddling the newly born River Ganges. 25kms or so down river is Haridwar, one of the sites of the quadrennial religious gathering known as the  urlLink Kumbha Mela ; the largest gathering of humanity on the planet. As both Haridwar and Rishikesh are holy towns, meat is forbidden. They are strictly vegetarian towns.  Rishikesh itself is split into two parts, the main town, which is a very hectic, typically Indian town, full of blaring horns and holy cows. But on the far side of the river there is a part of the town where cars cannot come, populated by ashrams and the occasional guest house. To get there, one must take a boat or cross  urlLink a lovely footbridge  (which was built by the British during WWII). On the peak of the hill on that quiet side of town, there is a very serene, and seemingly very ancient, town square, which I stumbled across late one afternoon in 1993.  The afternoon light was lovely, and there were many sahdus (Indian holy men) sitting around. One sahdu was standing, but leaning on a swing hanging from a large tree. 'He's been standing for nearly 10 years', one of the sahdus later told me proudly. 'He sleeps standing, he eats standing, he even shits standing'. Wow, I thought. Impressive.  I smoked a chillum with a few of the sahdus as the light went from yellow to gold and the afternoon shone in its final glory. Whether it was the surroundings, the light, or just the hash, I will never be sure (most likely it was a combination of all three), but I felt a connection to the human social condition that afternoon that I have never felt since. The symbolism of the town square was intensely powerful - particularly without the modern irritations of traffic, noise or advertising. People came and went about their business, as they purchased vegetables for the evening meal, or just sat and talked to their neighbours at the end of the day. I was very envious of the standing sahdu. It was certainly a nice place to hang around (groan..) for a decade. Not that I envied him his feat of religious asceticism.  Just the view.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "572235  3745396  ...         Back by popular demand is the third and final installment of Wisconsin Dells saga. It seems that one of my readers was not satisfied with the way I trailed off about the girls I met in the Dells. While I had more important things to do then meet with girls, I did spend a hefty amount of time with them.   The three girl's phone numbers I had previously announced I had are still in safe-keeping. I plan on calling 0 out of 3 of these girls and I will have nothing to do with any of them. The perfect girl in the pool, however, is a different story. There were two different pools in area that the guys and I swam in. Hence, there were two perfect girls; one in each pool. I apologize to the reader who had expected the number of pools to be at least ten so that there could be ten perfect girls in the world.   Anyways, while I'm on the subject, I did get to spend some time with one of the more attractive of the girls I met. She was a stunning dirty blonde with eyes that could make your legs weaken, lips that could make a grown man cry, and a smile that would melt the heart of any male. She had a cool demeanor and a strong aura of a rebel when I was near her. As much as I tried to get close to her, her independent aura made me back down like a kitten from a pit bull, afraid that she would reject me and leave. Luckily she did neither and we talked for what seemed like five minutes but in all actuality was more like two hours. I actually had the courage to ask her if she wanted to kiss...  'No.' she said. My heart sank. I was rejected. Well, it was worth a shot I had told myself. But then she added something that made my heart shoot back into my rib-cage. 'I can't. My mom is right behind the big dolphin.' Oh, thank God for the big dolphin. I had been saved by a six foot long plastic dolphin. I wanted nothing more than to hold her in my arms at the very least before I had to go, but it was cut short because I had to return to my hotel room for curfew.   Who knew that on one fateful night during a Wisconsin trip, I would meet the perfect girl. Someone to talk to, laugh with, and just have a good time with, without worrying about what anyone would say or do about it. I wish I could say I plan on meeting this girl again, but I don't. I do not see how that could be possible at all being in the great state of Indiana while my angel is in the equally as great, but very far away state of Minnesota.  The great thing is, I think Mariah felt the same about me. That's right, Mariah. Mariah Robinson. I leave you to ponder your own love story, reader. As I stroll off to bed wiping a single tear from my eye wondering myself, what could have been...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "228627  3297995  ...               You may recall that I had had pretty darn good vision after my recent cataract surgery, and I was (exceedingly) happy.  But my vision gradually degraded, as 50% of all cataract surgeries do, due to a clouding of the membrane that separates the lens from the back of the eye.  It was a gradual, but in the end, quite noticeable decline in acuity.  Well, I just got back from a quick outpatient procedure to correct this, and it couldn't have been more trouble-free.  To destroy the errant membrane, they shine multiple laser picobursts into your eye (about 20 to 30 of them), each one of which causes a pressure wave that punches a hole through the membrane.  Actually, a great metaphor that the surgeon used is that you should imagine a tightly-stretched film of Saran Wrap.   If you take a pin and pop a hole in the film, you'll get far more destruction than just a little hole, because the film is stretched so tightly.  So, I sat down, and he aimed his laser this way and that, popping the film with his laser bursts, until the membrane was essentially destroyed.  It was slightly startling, because I could FEEL the shock wave in the eye (like throwing a stone into a calm pond).  It took about three to five minutes - I didn't have to change out of my street clothes, even.  Many have asked, 'Don't you need all of your various body parts? Wasn't that part there for some reason?'  Well, it does more or less hold your lens in place, but the artificial lens that they put in during the cataract operation is sufficiently anchored down after 6 to 8 weeks that they can proceed with the destruction of the membrane.  And the results?  Amazing.  Using my evil eye, I after the operation, I could read the little badges on the cars with the brand name spelled out ('Mercury') from a distance of several car lengths.  And, for actual before-and-after comparison, I had put a large book (The Way To Cook, Julia Child, hardcover) against the piano and had seen how many paces away from the book I had to be before I couldn't read its title any more.  All tests were made with the evil eye open, and the excellent eye shut. Before the operation, WITH my glasses, I could just read the book title from about 8 feet away.  After the operation, WITHOUT my glasses, I could read the book from 10 feet away, and WITH my glasses, I could still read it from 23 feet away.  I might have been able to do a little better, but I ran out of living room.  So, almost a three-fold improvement in acuity, like going from 20/90 to 20/30.   Naturally, I'm thrilled.  (Even as I'm writing this just now, I can't help being tickled at how sharp everything looks).  I'm pretty confident now that even should my excellent eye suddenly have some horrible mishap, I would still easily qualify to drive with just my evil eye, especially if they would consent to clearing the freeways every morning and evening for my convenience.  And it's clearly good enough for me to be able to depend on it for reading and working, even should the other one fail.  The evil eye still has the dread macular pucker (the scar tissue on the retina, which makes things a bit wavy), and I'm still taking eye drops every day to prevent the retina from swelling, and my eye still LOOKS a little evil, but from the inside looking out, I'm deeply satisfied.  The retinal surgeon has no intention of operating on the macular pucker any time soon, so hopefully you won't hear from me about my dopey eye for a long, long time.  -Tom         \n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 420,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "BlogDf.sample(5) # Checking random 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "vCGAuhndF8y1",
    "outputId": "8de746c6-6145-4ff8-81a0-8127d61427cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173828</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173545</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink     ( urlLink via )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173544</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink     ( urlLink via )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173966</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173558</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink    ( urlLink via )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173587</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink    ( urlLink via )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173427</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173220</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173533</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink     ( urlLink via )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173515</th>\n",
       "      <td>8173</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  ...                                          text\n",
       "173828  8173  ...          urlLink                             \n",
       "173545  8173  ...          urlLink     ( urlLink via )         \n",
       "173544  8173  ...          urlLink     ( urlLink via )         \n",
       "173966  8173  ...          urlLink                             \n",
       "173558  8173  ...          urlLink    ( urlLink via )          \n",
       "173587  8173  ...          urlLink    ( urlLink via )          \n",
       "173427  8173  ...          urlLink                             \n",
       "173220  8173  ...          urlLink                             \n",
       "173533  8173  ...          urlLink     ( urlLink via )         \n",
       "173515  8173  ...          urlLink                             \n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if duplicate rows exist in the database\n",
    "duplicate = BlogDf[BlogDf.duplicated()]\n",
    "duplicate.sort_values('id', axis = 0, inplace=True)\n",
    "duplicate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlM4c9HSaPKR",
    "outputId": "034b6e9d-5a81-4500-dce6-7254f43f4424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset after duplicate rows removal (676598, 7)\n"
     ]
    }
   ],
   "source": [
    "# As we can see there are duplicate data as well. We shall remove the duplicates to remove unintended bias\n",
    "BlogDf.drop_duplicates(inplace=True)\n",
    "print('Shape of the dataset after duplicate rows removal', BlogDf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qzj2YKI5JTX"
   },
   "source": [
    "#### This is multi-class as well as multi-label case study. We need to understand each label and each class as well.\n",
    "* Label - age, gender, topic, sign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQIYnM5FKRvB",
    "outputId": "13b0b906-eadc-4139-812e-5c5391719088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class in this label- Gender: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "male      342272\n",
       "female    334326\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 423,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of class in this label- Gender:',BlogDf.gender.value_counts().count()) #Check each label # Capturing the gender category count\n",
    "BlogDf.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Mpvz_8XLAqo",
    "outputId": "31543083-c390-46bf-db40-a89cc2cfa936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class in this label -Topic: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indUnk                     248738\n",
       "Student                    153080\n",
       "Technology                 41913 \n",
       "Arts                       32305 \n",
       "Education                  29572 \n",
       "Communications-Media       20038 \n",
       "Internet                   15884 \n",
       "Non-Profit                 14618 \n",
       "Engineering                11561 \n",
       "Law                        9022  \n",
       "Publishing                 7710  \n",
       "Science                    7207  \n",
       "Government                 6866  \n",
       "Consulting                 5823  \n",
       "Religion                   5188  \n",
       "Fashion                    4824  \n",
       "Marketing                  4757  \n",
       "Advertising                4663  \n",
       "BusinessServices           4476  \n",
       "Banking                    4033  \n",
       "Chemicals                  3919  \n",
       "Telecommunications         3842  \n",
       "Accounting                 3703  \n",
       "Museums-Libraries          3095  \n",
       "Military                   3093  \n",
       "Sports-Recreation          3029  \n",
       "HumanResources             3003  \n",
       "RealEstate                 2867  \n",
       "Manufacturing              2231  \n",
       "Biotech                    2229  \n",
       "Transportation             2185  \n",
       "Tourism                    1936  \n",
       "LawEnforcement-Security    1875  \n",
       "Architecture               1611  \n",
       "InvestmentBanking          1286  \n",
       "Automotive                 1242  \n",
       "Agriculture                1234  \n",
       "Construction               1075  \n",
       "Environment                588   \n",
       "Maritime                   277   \n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 424,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check each label # Capturing the blogger topic category count # We have category unknown and student as high compared to others\n",
    "print('Number of class in this label -Topic:',BlogDf.topic.value_counts().count()) \n",
    "BlogDf.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvEqkV3ULb9L",
    "outputId": "c304396e-9cf5-4f6b-cb86-591649a621ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class in this label -Sign: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Aries          64758\n",
       "Cancer         64709\n",
       "Libra          62146\n",
       "Taurus         61393\n",
       "Virgo          60200\n",
       "Scorpio        56597\n",
       "Pisces         53657\n",
       "Leo            53603\n",
       "Gemini         51309\n",
       "Sagittarius    49831\n",
       "Aquarius       49422\n",
       "Capricorn      48973\n",
       "Name: sign, dtype: int64"
      ]
     },
     "execution_count": 425,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check each label # Capturing the blogger sign category count - fairly distributed\n",
    "print('Number of class in this label -Sign:',BlogDf.sign.value_counts().count()) \n",
    "BlogDf.sign.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5hHlXDwL4Hp",
    "outputId": "8f04c16c-d372-4246-e9db-3d1bd077abdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class in this label -Age: 26\n",
      "Blog from minimum age group is: 13 and Blog from maximum age group is: 48\n",
      "Blog from age group < 10: 0\n",
      "Blog from age group 11 < 20: 233789\n",
      "Blog from age group 21 < 30: 319817\n",
      "Blog from age group 31 < 40: 97229\n",
      "Blog from age group 41 < 50: 25763\n",
      "Blog from age group >50: 0\n"
     ]
    }
   ],
   "source": [
    "# Capturing the blogger age category count - young group are higher\n",
    "print('Number of class in this label -Age:',BlogDf.age.value_counts().count()) \n",
    "print('Blog from minimum age group is:', BlogDf.age.min(), 'and Blog from maximum age group is:', BlogDf.age.max())\n",
    "print('Blog from age group < 10:', BlogDf.age[BlogDf['age']<= 10].value_counts().sum())\n",
    "print('Blog from age group 11 < 20:', BlogDf.age[(BlogDf['age']<= 20) & (BlogDf['age']> 10)].value_counts().sum())\n",
    "print('Blog from age group 21 < 30:',BlogDf.age[(BlogDf['age']<= 30) & (BlogDf['age']> 20)].value_counts().sum())\n",
    "print('Blog from age group 31 < 40:', BlogDf.age[(BlogDf['age']<= 40) & (BlogDf['age']> 30)].value_counts().sum())\n",
    "print('Blog from age group 41 < 50:', BlogDf.age[(BlogDf['age']<= 50) & (BlogDf['age']> 40)].value_counts().sum())\n",
    "print('Blog from age group >50:', BlogDf.age[(BlogDf['age']> 50)].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "id": "459PX0CR8Fqo"
   },
   "outputs": [],
   "source": [
    "#Create a age group as label instead of 26 different class\n",
    "BlogDf.loc[(BlogDf['age'] >= 10) & (BlogDf['age'] <20), 'Age'] = 10\n",
    "BlogDf.loc[(BlogDf['age'] >= 20) & (BlogDf['age'] <30), 'Age'] = 20\n",
    "BlogDf.loc[(BlogDf['age'] >= 30) & (BlogDf['age'] <40), 'Age'] = 30\n",
    "BlogDf.loc[(BlogDf['age'] >= 40) & (BlogDf['age'] <50), 'Age'] = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "id": "eyPDzAA7ZUDt"
   },
   "outputs": [],
   "source": [
    "BlogDf.drop(['id','date','age'], axis=1, inplace=True) # Remove Id and Date which is not relevant for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1uOC9pN_-Ex",
    "outputId": "6d48d393-8684-48a9-9ae4-315d5e41874c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'topic', 'sign', 'text', 'Age'], dtype='object')"
      ]
     },
     "execution_count": 429,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlogDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoOL87n7G3ch"
   },
   "source": [
    "#### Step2: Perform data pre-processing on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kkPVNVMSLEJ"
   },
   "source": [
    "* Data cleansing by removing unwanted characters, spaces, stop words etc. Convert text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "id": "K0oEpX58lndg"
   },
   "outputs": [],
   "source": [
    "BlogDf['clean_text']=BlogDf['text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x)) #remove non-alphabets charaacters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "id": "GlVvkPHUlnfq"
   },
   "outputs": [],
   "source": [
    "BlogDf['clean_text']=BlogDf['clean_text'].apply(lambda x: x.lower())  #Convert Uppercase text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "id": "FCtvBzv8pOZG"
   },
   "outputs": [],
   "source": [
    "BlogDf['clean_text']=BlogDf['clean_text'].apply(lambda x: x.strip()) #remove begining and trailing space characters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "DvVSuREPpg95",
    "outputId": "68f1a7ac-f6cd-4028-c56b-7ab31a470b5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>text</th>\n",
       "      <th>Age</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>I surf the English news sites a lot looking for tidbits on Korea and how foreigners (like me) view the 'Hermit Kingdom' but also as a way to keep up with this fast-moving place.  Sometimes, though, one needs to check the veracity of the figures put in the papers...especially the local ones.  Here are two examples of how the English version of the Korea Times and that of the JoongAng Ilbo (Daily).  The first is pretty straightforward.   urlLink Korea Times  said that 249 people were arrested for forging Korean passports, but  urlLink JoongAng Ilbo  says just 114 were accused.  Huh?  Another one:  urlLink JoongAng Ilbo  said that S&amp;P is positive on Korean banks (a good thing), while the  urlLink Korea Times  said that S&amp;P was a tad worried about the bad loans that banks extended to small and medium-sized firms.  I have no idea why the simple facts seem to be presented so differently...it can't simply be translation, can it?</td>\n",
       "      <td>30.0</td>\n",
       "      <td>i surf the english news sites a lot looking for tidbits on korea and how foreigners like me view the hermit kingdom but also as a way to keep up with this fast moving place sometimes though one needs to check the veracity of the figures put in the papers especially the local ones here are two examples of how the english version of the korea times and that of the joongang ilbo daily the first is pretty straightforward urllink korea times said that people were arrested for forging korean passports but urllink joongang ilbo says just were accused huh another one urllink joongang ilbo said that s p is positive on korean banks a good thing while the urllink korea times said that s p was a tad worried about the bad loans that banks extended to small and medium sized firms i have no idea why the simple facts seem to be presented so differently it can t simply be translation can it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              clean_text\n",
       "9  male   ...  i surf the english news sites a lot looking for tidbits on korea and how foreigners like me view the hermit kingdom but also as a way to keep up with this fast moving place sometimes though one needs to check the veracity of the figures put in the papers especially the local ones here are two examples of how the english version of the korea times and that of the joongang ilbo daily the first is pretty straightforward urllink korea times said that people were arrested for forging korean passports but urllink joongang ilbo says just were accused huh another one urllink joongang ilbo said that s p is positive on korean banks a good thing while the urllink korea times said that s p was a tad worried about the bad loans that banks extended to small and medium sized firms i have no idea why the simple facts seem to be presented so differently it can t simply be translation can it\n",
       "\n",
       "[1 rows x 6 columns]"
      ]
     },
     "execution_count": 433,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlogDf[9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POPgvzO2sDO6",
    "outputId": "00ae03c6-fbbd-4a67-9ffe-f8ad843c641e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # Download stopword corpus\n",
    "nltk.download('stopwords')\n",
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "id": "qFW-d0rXs2fO"
   },
   "outputs": [],
   "source": [
    "#Remove all stopwords\n",
    "BlogDf['clean_text']=BlogDf['clean_text'].apply(lambda x: ' '.join([words for words in x.split() if words not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "IXjZ15M5tfdj",
    "outputId": "23de9d01-deeb-4c98-ba80-11bbce09108e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>text</th>\n",
       "      <th>Age</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>I surf the English news sites a lot looking for tidbits on Korea and how foreigners (like me) view the 'Hermit Kingdom' but also as a way to keep up with this fast-moving place.  Sometimes, though, one needs to check the veracity of the figures put in the papers...especially the local ones.  Here are two examples of how the English version of the Korea Times and that of the JoongAng Ilbo (Daily).  The first is pretty straightforward.   urlLink Korea Times  said that 249 people were arrested for forging Korean passports, but  urlLink JoongAng Ilbo  says just 114 were accused.  Huh?  Another one:  urlLink JoongAng Ilbo  said that S&amp;P is positive on Korean banks (a good thing), while the  urlLink Korea Times  said that S&amp;P was a tad worried about the bad loans that banks extended to small and medium-sized firms.  I have no idea why the simple facts seem to be presented so differently...it can't simply be translation, can it?</td>\n",
       "      <td>30.0</td>\n",
       "      <td>surf english news sites lot looking tidbits korea foreigners like view hermit kingdom also way keep fast moving place sometimes though one needs check veracity figures put papers especially local ones two examples english version korea times joongang ilbo daily first pretty straightforward urllink korea times said people arrested forging korean passports urllink joongang ilbo says accused huh another one urllink joongang ilbo said p positive korean banks good thing urllink korea times said p tad worried bad loans banks extended small medium sized firms idea simple facts seem presented differently simply translation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      clean_text\n",
       "9  male   ...  surf english news sites lot looking tidbits korea foreigners like view hermit kingdom also way keep fast moving place sometimes though one needs check veracity figures put papers especially local ones two examples english version korea times joongang ilbo daily first pretty straightforward urllink korea times said people arrested forging korean passports urllink joongang ilbo says accused huh another one urllink joongang ilbo said p positive korean banks good thing urllink korea times said p tad worried bad loans banks extended small medium sized firms idea simple facts seem presented differently simply translation\n",
       "\n",
       "[1 rows x 6 columns]"
      ]
     },
     "execution_count": 436,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlogDf[9:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z7za1Rr5vme"
   },
   "source": [
    "* Target/label merger and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "mbnM3bbG43Y0"
   },
   "outputs": [],
   "source": [
    "BlogDf['Age']=BlogDf['Age'].astype('object') # To treat age as object and not numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "id": "44F78_JfzJFt"
   },
   "outputs": [],
   "source": [
    "# Target / Label merger\n",
    "BlogDf['labels']=BlogDf.apply(lambda col: [col['gender'],str(col['Age']),col['topic'],col['sign']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "id": "kf68Q7in6uCb"
   },
   "outputs": [],
   "source": [
    "BlogDf=BlogDf[['clean_text','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "0vldSCTL6uEN",
    "outputId": "f8b6ee7a-0a3d-4a0c-f6cb-321a26ecf29d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>surf english news sites lot looking tidbits korea foreigners like view hermit kingdom also way keep fast moving place sometimes though one needs check veracity figures put papers especially local ones two examples english version korea times joongang ilbo daily first pretty straightforward urllink korea times said people arrested forging korean passports urllink joongang ilbo says accused huh another one urllink joongang ilbo said p positive korean banks good thing urllink korea times said p tad worried bad loans banks extended small medium sized firms idea simple facts seem presented differently simply translation</td>\n",
       "      <td>[male, 30.0, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       clean_text                                     labels\n",
       "9  surf english news sites lot looking tidbits korea foreigners like view hermit kingdom also way keep fast moving place sometimes though one needs check veracity figures put papers especially local ones two examples english version korea times joongang ilbo daily first pretty straightforward urllink korea times said people arrested forging korean passports urllink joongang ilbo says accused huh another one urllink joongang ilbo said p positive korean banks good thing urllink korea times said p tad worried bad loans banks extended small medium sized firms idea simple facts seem presented differently simply translation  [male, 30.0, InvestmentBanking, Aquarius]"
      ]
     },
     "execution_count": 440,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlogDf[9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "MJa4wP6ZcseA",
    "outputId": "aa85ab46-53dc-4ff7-d0c3-16d8044e77f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f52fc5f1f10>"
      ]
     },
     "execution_count": 441,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = BlogDf.clean_text.str.len()\n",
    "lens.hist(bins = np.arange(0,5000,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "id": "GHr8X82ZZTpL"
   },
   "outputs": [],
   "source": [
    "# Using MultiLabelBinarizer transformer to transform labels\n",
    "label_counts=dict()\n",
    "\n",
    "for labels in BlogDf.labels.values:\n",
    "    for label in labels:\n",
    "        if label in label_counts:\n",
    "            label_counts[label]+=1\n",
    "        else:\n",
    "            label_counts[label]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-h0FgriZliB",
    "outputId": "e49c8282-8073-4f06-b051-f7a7ac57047a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10.0': 233789,\n",
       " '20.0': 319817,\n",
       " '30.0': 92231,\n",
       " '40.0': 30761,\n",
       " 'Accounting': 3703,\n",
       " 'Advertising': 4663,\n",
       " 'Agriculture': 1234,\n",
       " 'Aquarius': 49422,\n",
       " 'Architecture': 1611,\n",
       " 'Aries': 64758,\n",
       " 'Arts': 32305,\n",
       " 'Automotive': 1242,\n",
       " 'Banking': 4033,\n",
       " 'Biotech': 2229,\n",
       " 'BusinessServices': 4476,\n",
       " 'Cancer': 64709,\n",
       " 'Capricorn': 48973,\n",
       " 'Chemicals': 3919,\n",
       " 'Communications-Media': 20038,\n",
       " 'Construction': 1075,\n",
       " 'Consulting': 5823,\n",
       " 'Education': 29572,\n",
       " 'Engineering': 11561,\n",
       " 'Environment': 588,\n",
       " 'Fashion': 4824,\n",
       " 'Gemini': 51309,\n",
       " 'Government': 6866,\n",
       " 'HumanResources': 3003,\n",
       " 'Internet': 15884,\n",
       " 'InvestmentBanking': 1286,\n",
       " 'Law': 9022,\n",
       " 'LawEnforcement-Security': 1875,\n",
       " 'Leo': 53603,\n",
       " 'Libra': 62146,\n",
       " 'Manufacturing': 2231,\n",
       " 'Maritime': 277,\n",
       " 'Marketing': 4757,\n",
       " 'Military': 3093,\n",
       " 'Museums-Libraries': 3095,\n",
       " 'Non-Profit': 14618,\n",
       " 'Pisces': 53657,\n",
       " 'Publishing': 7710,\n",
       " 'RealEstate': 2867,\n",
       " 'Religion': 5188,\n",
       " 'Sagittarius': 49831,\n",
       " 'Science': 7207,\n",
       " 'Scorpio': 56597,\n",
       " 'Sports-Recreation': 3029,\n",
       " 'Student': 153080,\n",
       " 'Taurus': 61393,\n",
       " 'Technology': 41913,\n",
       " 'Telecommunications': 3842,\n",
       " 'Tourism': 1936,\n",
       " 'Transportation': 2185,\n",
       " 'Virgo': 60200,\n",
       " 'female': 334326,\n",
       " 'indUnk': 248738,\n",
       " 'male': 342272}"
      ]
     },
     "execution_count": 443,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "id": "3NhV8T9bbEsk"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "binarizer=MultiLabelBinarizer(classes=sorted(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "id": "IxXlh5qCbUNR"
   },
   "outputs": [],
   "source": [
    "label_T=binarizer.fit_transform(BlogDf.labels) #transformed labels for use in model classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REbtbFBqkpej",
    "outputId": "5631cc0e-d4fa-4146-c34d-5e5a4ae473c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10.0', '20.0', '30.0', '40.0', 'Accounting', 'Advertising',\n",
       "       'Agriculture', 'Aquarius', 'Architecture', 'Aries', 'Arts',\n",
       "       'Automotive', 'Banking', 'Biotech', 'BusinessServices', 'Cancer',\n",
       "       'Capricorn', 'Chemicals', 'Communications-Media', 'Construction',\n",
       "       'Consulting', 'Education', 'Engineering', 'Environment', 'Fashion',\n",
       "       'Gemini', 'Government', 'HumanResources', 'Internet',\n",
       "       'InvestmentBanking', 'Law', 'LawEnforcement-Security', 'Leo',\n",
       "       'Libra', 'Manufacturing', 'Maritime', 'Marketing', 'Military',\n",
       "       'Museums-Libraries', 'Non-Profit', 'Pisces', 'Publishing',\n",
       "       'RealEstate', 'Religion', 'Sagittarius', 'Science', 'Scorpio',\n",
       "       'Sports-Recreation', 'Student', 'Taurus', 'Technology',\n",
       "       'Telecommunications', 'Tourism', 'Transportation', 'Virgo',\n",
       "       'female', 'indUnk', 'male'], dtype=object)"
      ]
     },
     "execution_count": 446,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zS6AbZKdllkM",
    "outputId": "6f1a9c1c-ddf6-474c-b93d-57a369862e09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=['10.0', '20.0', '30.0', '40.0', 'Accounting',\n",
       "                             'Advertising', 'Agriculture', 'Aquarius',\n",
       "                             'Architecture', 'Aries', 'Arts', 'Automotive',\n",
       "                             'Banking', 'Biotech', 'BusinessServices', 'Cancer',\n",
       "                             'Capricorn', 'Chemicals', 'Communications-Media',\n",
       "                             'Construction', 'Consulting', 'Education',\n",
       "                             'Engineering', 'Environment', 'Fashion', 'Gemini',\n",
       "                             'Government', 'HumanResources', 'Internet',\n",
       "                             'InvestmentBanking', ...],\n",
       "                    sparse_output=False)"
      ]
     },
     "execution_count": 447,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XokQrNI4fUeo"
   },
   "source": [
    "* Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "id": "WMnMowHF8vD-"
   },
   "outputs": [],
   "source": [
    "X = BlogDf['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HT489CkJ_3VQ",
    "outputId": "6ec4a07f-d3db-4dd1-f4bc-2415f151a8fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676598,)"
      ]
     },
     "execution_count": 449,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "id": "mdAL2jNJLwRN"
   },
   "outputs": [],
   "source": [
    "X_N = X[:4000] # We can use the subset of the complete data to build, train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "id": "Elkchd0ieAk6"
   },
   "outputs": [],
   "source": [
    "# Using CountVectorizer with unigrams, bigrams and trigrams transformation of text features\n",
    "# This will help get us contextual information using bigram\n",
    "counter = CountVectorizer(stop_words = \"english\",ngram_range = (1,2),min_df = 1)\n",
    "Features_N = counter.fit_transform(X_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R43TpKW3kWpr",
    "outputId": "a8528126-6edb-4be4-a63d-2d4d995b4d1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 243510)"
      ]
     },
     "execution_count": 452,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features_N.shape #(ngram_range=(1,2), min_df=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "id": "YvMPNyuQ9wW8"
   },
   "outputs": [],
   "source": [
    "label_N = label_T[:4000] # Subset of the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVJFupPU95W6",
    "outputId": "a87f95aa-056d-4225-c382-4f4ffe218db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 243510)\n",
      "(4000, 58)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataset for the model\n",
    "print(Features_N.shape)\n",
    "print(label_N.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBeEWqt49rbZ"
   },
   "source": [
    "* Train, Test split for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "id": "08nVSy3scumV"
   },
   "outputs": [],
   "source": [
    "# Using CountVectorrizer with unigrams and bigrams transformation of text features\n",
    "Xtrain,Xtest,Ytrain,Ytest = train_test_split(Features_N,label_N, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWdS2LRhgG0L",
    "outputId": "ec99f6fe-be12-47f8-8a13-4529aefa0b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Xtrain: (3200, 243510)\n",
      "Shape of Xtest: (800, 243510)\n",
      "Shape of Ytrain: (3200, 58)\n",
      "Shape of Ytest: (800, 58)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Xtrain:', Xtrain.shape)\n",
    "print('Shape of Xtest:', Xtest.shape)\n",
    "print('Shape of Ytrain:', Ytrain.shape)\n",
    "print('Shape of Ytest:', Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nPz_bR2fgxZ"
   },
   "source": [
    "#### Step3: Design, train, tune and test the best text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "id": "hC_5_26Hhehc"
   },
   "outputs": [],
   "source": [
    "# generic function to print performance for different classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Dataframe to display classifier results\n",
    "performance_df = pd.DataFrame(columns = ['Classifier', 'Accuracy_Score', 'F1_score', 'Avg_Precision_Score','Avg_Recall_Score'])\n",
    "\n",
    "def display_score(clf):\n",
    "  print(clf ,' classifier with OneVsRest classification startegy:')\n",
    "  acc = np.round(accuracy_score(Ytest, Ypred)*100,2)\n",
    "  f1 = np.round(f1_score(Ytest, Ypred, average='micro')*100,2)\n",
    "  pre = np.round(average_precision_score(Ytest, Ypred, average='micro')*100,2)\n",
    "  rec = np.round(recall_score(Ytest, Ypred, average='micro')*100,2)\n",
    "  print('Accuracy score:', acc)\n",
    "  print('F1 score: ' , f1)\n",
    "  print('Average precision score: ', pre)\n",
    "  print('Average recall score: ', rec)\n",
    "  return acc,f1,pre,rec\n",
    "\n",
    "  #print('Accuracy score: %0.2f' %(accuracy_score(Ytest, Ypred)*100))\n",
    "  #print('F1 score: %0.2f' %(f1_score(Ytest, Ypred, average='micro')*100))\n",
    "  #print('Average precision score: %0.2f' %(average_precision_score(Ytest, Ypred, average='micro')*100))\n",
    "  #print('Average recall score: %0.2f' %(recall_score(Ytest, Ypred, average='micro')*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbxJJGSr9KJp",
    "outputId": "145d90ea-d8ee-40ca-d8e6-031d153f9a05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=1000,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 458,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Logistic Regression classifier\n",
    "# Since we have multilabel case, we shall use addiotional strategy available from sklearn referred as OneVsRest Classifier \n",
    "# It helps to classify each label-class from the rest.\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression( solver = 'lbfgs',max_iter = 1000)\n",
    "lr_model=OneVsRestClassifier(lr)\n",
    "lr_model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtkoDLf67_-Y",
    "outputId": "14a3d7ee-9ad5-4c5a-ea60-cb1799694672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression  classifier with OneVsRest classification startegy:\n",
      "Accuracy score: 59.25\n",
      "F1 score:  77.89\n",
      "Average precision score:  62.69\n",
      "Average recall score:  73.66\n"
     ]
    }
   ],
   "source": [
    "Ypred=lr_model.predict(Xtest)\n",
    "clf = 'Logistic Regression'\n",
    "a,b,c,d = display_score(clf)\n",
    "new_row = {'Classifier': str(clf), 'Accuracy_Score': a,'F1_score': b,'Avg_Precision_Score': c, 'Avg_Recall_Score': d}\n",
    "performance_df = performance_df.append(new_row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxyhlMjf80AQ",
    "outputId": "0c6cd833-32db-4156-853d-e5f13a33688c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                        fit_intercept=True, intercept_scaling=1,\n",
       "                                        loss='squared_hinge', max_iter=1000,\n",
       "                                        multi_class='ovr', penalty='l2',\n",
       "                                        random_state=None, tol=0.0001,\n",
       "                                        verbose=0),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 460,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm_model=OneVsRestClassifier(svm)\n",
    "svm_model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L--cy3jAePBS",
    "outputId": "76ceed15-9cb4-46ce-dd6d-622672e76b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector  classifier with OneVsRest classification startegy:\n",
      "Accuracy score: 57.63\n",
      "F1 score:  76.75\n",
      "Average precision score:  60.79\n",
      "Average recall score:  73.97\n"
     ]
    }
   ],
   "source": [
    "Ypred=svm_model.predict(Xtest)\n",
    "clf = 'Support Vector'\n",
    "a,b,c,d = display_score(clf)\n",
    "new_row = {'Classifier': str(clf), 'Accuracy_Score': a,'F1_score': b,'Avg_Precision_Score': c, 'Avg_Recall_Score': d}\n",
    "performance_df = performance_df.append(new_row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hjC_9kp80GO",
    "outputId": "507aff80-e9f5-44a2-fdc2-ab1466e883bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                            fit_prior=True),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 462,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb_model=OneVsRestClassifier(nb)\n",
    "nb_model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce63WNiUeavu",
    "outputId": "a306080e-a72f-4fab-bde7-5f8f89d496b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes  classifier with OneVsRest classification startegy:\n",
      "Accuracy score: 41.75\n",
      "F1 score:  72.48\n",
      "Average precision score:  55.02\n",
      "Average recall score:  67.69\n"
     ]
    }
   ],
   "source": [
    "Ypred=nb_model.predict(Xtest)\n",
    "clf = 'Naive Bayes'\n",
    "a,b,c,d = display_score(clf)\n",
    "new_row = {'Classifier': str(clf), 'Accuracy_Score': a,'F1_score': b,'Avg_Precision_Score': c, 'Avg_Recall_Score': d}\n",
    "performance_df = performance_df.append(new_row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDLtf1nEr6lb",
    "outputId": "33bf52e8-d8f4-4aae-cad0-3176ccb59609"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SGDClassifier(alpha=0.0001, average=False,\n",
       "                                            class_weight=None,\n",
       "                                            early_stopping=False, epsilon=0.1,\n",
       "                                            eta0=0.0, fit_intercept=True,\n",
       "                                            l1_ratio=0.15,\n",
       "                                            learning_rate='optimal',\n",
       "                                            loss='hinge', max_iter=1000,\n",
       "                                            n_iter_no_change=5, n_jobs=None,\n",
       "                                            penalty='l2', power_t=0.5,\n",
       "                                            random_state=None, shuffle=True,\n",
       "                                            tol=0.001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 464,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "sgd_model=OneVsRestClassifier(sgd)\n",
    "sgd_model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqgXpryBr8pX",
    "outputId": "d75cabf7-74a6-4228-a9d2-c8e4e76109e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD  classifier with OneVsRest classification startegy:\n",
      "Accuracy score: 57.0\n",
      "F1 score:  79.05\n",
      "Average precision score:  64.51\n",
      "Average recall score:  74.34\n"
     ]
    }
   ],
   "source": [
    "Ypred=sgd_model.predict(Xtest)\n",
    "clf = 'SGD'\n",
    "a,b,c,d = display_score(clf)\n",
    "new_row = {'Classifier': str(clf), 'Accuracy_Score': a,'F1_score': b,'Avg_Precision_Score': c, 'Avg_Recall_Score': d}\n",
    "performance_df = performance_df.append(new_row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIvn-HG6fLrF",
    "outputId": "0ec6d4aa-7aab-4ba7-8ed2-2158078c09aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                                 base_estimator=None,\n",
       "                                                 learning_rate=1.0,\n",
       "                                                 n_estimators=50,\n",
       "                                                 random_state=0),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 468,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=50, random_state=0)\n",
    "ada_model=OneVsRestClassifier(ada)\n",
    "ada_model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VTB7qQqj-Vp",
    "outputId": "f05d2296-9d03-4293-8855-5b40a59e8593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost  classifier with OneVsRest classification startegy:\n",
      "Accuracy score: 52.12\n",
      "F1 score:  77.42\n",
      "Average precision score:  61.85\n",
      "Average recall score:  74.09\n"
     ]
    }
   ],
   "source": [
    "Ypred=ada_model.predict(Xtest)\n",
    "clf = 'AdaBoost'\n",
    "a,b,c,d = display_score(clf)\n",
    "new_row = {'Classifier': str(clf), 'Accuracy_Score': a,'F1_score': b,'Avg_Precision_Score': c, 'Avg_Recall_Score': d}\n",
    "performance_df = performance_df.append(new_row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-04DA8wme-MI",
    "outputId": "bb9b3839-9158-4f43-c197-848750fc4306"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy_Score</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Avg_Precision_Score</th>\n",
       "      <th>Avg_Recall_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>59.25</td>\n",
       "      <td>77.89</td>\n",
       "      <td>62.69</td>\n",
       "      <td>73.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Support Vector</td>\n",
       "      <td>57.63</td>\n",
       "      <td>76.75</td>\n",
       "      <td>60.79</td>\n",
       "      <td>73.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>41.75</td>\n",
       "      <td>72.48</td>\n",
       "      <td>55.02</td>\n",
       "      <td>67.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>57.00</td>\n",
       "      <td>79.05</td>\n",
       "      <td>64.51</td>\n",
       "      <td>74.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>52.12</td>\n",
       "      <td>77.42</td>\n",
       "      <td>61.85</td>\n",
       "      <td>74.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Accuracy_Score  ...  Avg_Precision_Score  Avg_Recall_Score\n",
       "0  Logistic Regression  59.25           ...  62.69                73.66           \n",
       "1  Support Vector       57.63           ...  60.79                73.97           \n",
       "2  Naive Bayes          41.75           ...  55.02                67.69           \n",
       "3  SGD                  57.00           ...  64.51                74.34           \n",
       "4  AdaBoost             52.12           ...  61.85                74.09           \n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 474,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVquMnY-kg0c"
   },
   "source": [
    "We have SGD classifier better results than other classifiers with 79% f1 score and best precision and recall score as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzH5ZaAvOsb"
   },
   "source": [
    "#### Step4: Display and explain detail the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "id": "DWeMYNebk4XZ"
   },
   "outputs": [],
   "source": [
    "Ypred=sgd_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "is83y1xSvZUk",
    "outputId": "561e6b6c-3fe6-46c9-98ed-7abc76500080"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.584507</td>\n",
       "      <td>0.688797</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>0.853480</td>\n",
       "      <td>0.915521</td>\n",
       "      <td>0.883412</td>\n",
       "      <td>509.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aquarius</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.556522</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aries</th>\n",
       "      <td>0.833992</td>\n",
       "      <td>0.899787</td>\n",
       "      <td>0.865641</td>\n",
       "      <td>469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arts</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banking</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusinessServices</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cancer</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Capricorn</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Communications-Media</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engineering</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internet</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InvestmentBanking</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leo</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Libra</th>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Profit</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pisces</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sagittarius</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scorpio</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports-Recreation</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taurus</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technology</th>\n",
       "      <td>0.851464</td>\n",
       "      <td>0.912556</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgo</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>0.809917</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.618297</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indUnk</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.280303</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.882171</td>\n",
       "      <td>0.942053</td>\n",
       "      <td>0.911129</td>\n",
       "      <td>604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.843916</td>\n",
       "      <td>0.743437</td>\n",
       "      <td>0.790497</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.413740</td>\n",
       "      <td>0.237244</td>\n",
       "      <td>0.280304</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.833118</td>\n",
       "      <td>0.743437</td>\n",
       "      <td>0.762452</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.822146</td>\n",
       "      <td>0.743437</td>\n",
       "      <td>0.767316</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      precision    recall  f1-score  support\n",
       "10.0                  0.838384   0.584507  0.688797  142.0  \n",
       "20.0                  0.714286   0.492958  0.583333  142.0  \n",
       "30.0                  0.853480   0.915521  0.883412  509.0  \n",
       "40.0                  0.000000   0.000000  0.000000  7.0    \n",
       "Aquarius              0.695652   0.463768  0.556522  69.0   \n",
       "Aries                 0.833992   0.899787  0.865641  469.0  \n",
       "Arts                  1.000000   0.333333  0.500000  3.0    \n",
       "Banking               0.000000   0.000000  0.000000  4.0    \n",
       "BusinessServices      1.000000   0.555556  0.714286  9.0    \n",
       "Cancer                0.750000   0.272727  0.400000  22.0   \n",
       "Capricorn             0.857143   0.857143  0.857143  14.0   \n",
       "Communications-Media  0.000000   0.000000  0.000000  8.0    \n",
       "Education             1.000000   0.200000  0.333333  20.0   \n",
       "Engineering           1.000000   0.266667  0.421053  15.0   \n",
       "Gemini                1.000000   0.100000  0.181818  10.0   \n",
       "Internet              1.000000   0.500000  0.666667  2.0    \n",
       "InvestmentBanking     1.000000   0.913043  0.954545  23.0   \n",
       "Leo                   1.000000   0.320000  0.484848  25.0   \n",
       "Libra                 0.864865   0.484848  0.621359  66.0   \n",
       "Non-Profit            0.750000   0.250000  0.375000  12.0   \n",
       "Pisces                0.600000   0.272727  0.375000  11.0   \n",
       "Sagittarius           1.000000   0.037037  0.071429  27.0   \n",
       "Science               0.750000   0.600000  0.666667  5.0    \n",
       "Scorpio               0.900000   0.290323  0.439024  62.0   \n",
       "Sports-Recreation     1.000000   0.785714  0.880000  14.0   \n",
       "Student               0.805556   0.542056  0.648045  107.0  \n",
       "Taurus                0.500000   0.187500  0.272727  16.0   \n",
       "Technology            0.851464   0.912556  0.880952  446.0  \n",
       "Virgo                 0.000000   0.000000  0.000000  9.0    \n",
       "female                0.809917   0.500000  0.618297  196.0  \n",
       "indUnk                0.740000   0.280303  0.406593  132.0  \n",
       "male                  0.882171   0.942053  0.911129  604.0  \n",
       "micro avg             0.843916   0.743437  0.790497  3200.0 \n",
       "macro avg             0.413740   0.237244  0.280304  3200.0 \n",
       "weighted avg          0.833118   0.743437  0.762452  3200.0 \n",
       "samples avg           0.822146   0.743437  0.767316  3200.0 "
      ]
     },
     "execution_count": 476,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Ytest, Ypred, target_names=binarizer.classes_, output_dict=True)\n",
    "pd.set_option('display.max_rows', 100) \n",
    "df = pd.DataFrame(report).transpose()\n",
    "df[df['support'] != 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYZ4Gy1MlOk1"
   },
   "source": [
    "Precision  gives the percentage of correctness of the classified positive instances and recall gives the percentage of correctness of the actual positive instances. To focus on precision or recall it depends upon the business case. For example for heatlth related cases recall is more important performance metric than precision. However in our case precision would be enough.\n",
    "\n",
    "For age classification '10 to 20' and '30 to 40' we have more than 80% precision. however the model poorly performed '>40' and '20 to 30'. This could be due to imbalance in dataset when we chose 4000 records from the actual data set. A good subset of the data which includes balanced class labels would have given much better results for model to learn.\n",
    "\n",
    "For gender classification the model was able to perform better in classifing precisely as male or female author.\n",
    "\n",
    "For sign and industry, the mddel performed with better precision in most cases except for few which could be class imbalance as mentioned above while choosing the data subset.\n",
    "\n",
    "We can also use f1 score which uses weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. As a rule of thumb, the weighted average of F1 should be used to compare classifier models instead of global accuracy. The f1 score is 80% using SDG classifier.\n",
    "\n",
    "We have micro average and macro avergae metric as well. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In  our multi-class classification, micro-average is preferable since we see there is a class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpee_KmPvUAT"
   },
   "source": [
    "#### Step5: Print the true vs predicted labels for any 5 entries from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6P1gxMhsvlDL",
    "outputId": "d2271cd8-b026-4d88-99af-863c5fd0754f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** For  430 th entry ***************\n",
      "Predicted: ('20.0', 'female', 'male')\n",
      "Actual   : ('20.0', 'Aries', 'Internet', 'male')\n",
      "*************** For  618 th entry ***************\n",
      "Predicted: ('30.0', 'Aries', 'Technology', 'male')\n",
      "Actual   : ('30.0', 'Aries', 'Technology', 'male')\n",
      "*************** For  531 th entry ***************\n",
      "Predicted: ('Student', 'female')\n",
      "Actual   : ('20.0', 'Sagittarius', 'female', 'indUnk')\n",
      "*************** For  671 th entry ***************\n",
      "Predicted: ('30.0', 'Aries', 'Technology', 'male')\n",
      "Actual   : ('30.0', 'Aries', 'Technology', 'male')\n",
      "*************** For  455 th entry ***************\n",
      "Predicted: ('30.0', 'Aries', 'Technology', 'male')\n",
      "Actual   : ('30.0', 'Aries', 'Technology', 'male')\n"
     ]
    }
   ],
   "source": [
    "Ypred_inverse = binarizer.inverse_transform(Ypred)\n",
    "Ytest_inverse = binarizer.inverse_transform(Ytest)\n",
    "arr = np.random.randint(800, size=(1, 5))\n",
    "for i in range(0,5):\n",
    "  j = arr[0,i]\n",
    "  print('*************** For ', j, 'th entry ***************')\n",
    "  print('Predicted:',Ypred_inverse[j])\n",
    "  print('Actual   :',Ytest_inverse [j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKFw80aOswbB"
   },
   "source": [
    "Of randomly selected five test instances, we see the model gives predicted and actual label same for three cases index - 618, 671 and 455. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt1lX6RYvPkY"
   },
   "source": [
    "### Project2 - Python based interactive semi - rule based chatbot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1qzHc5bKFEd"
   },
   "source": [
    "#### Project Task: \n",
    "1. Start chat session with greetings and ask what the user is looking for.\n",
    "2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus.\n",
    "3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht24IEJiGthZ"
   },
   "source": [
    "* Import json and read json corpus file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UdyKh3DEMLg",
    "outputId": "a2570582-2a52-4f61-f6d6-25e304e1ebe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'context_set': '',\n",
       "   'patterns': ['hi',\n",
       "    'how are you',\n",
       "    'is anyone there',\n",
       "    'hello',\n",
       "    'whats up',\n",
       "    'hey',\n",
       "    'yo',\n",
       "    'listen',\n",
       "    'please help me',\n",
       "    'i am learner from',\n",
       "    'i belong to',\n",
       "    'aiml batch',\n",
       "    'aifl batch',\n",
       "    'i am from',\n",
       "    'my pm is',\n",
       "    'blended',\n",
       "    'online',\n",
       "    'i am from',\n",
       "    'hey ya',\n",
       "    'talking to you for first time'],\n",
       "   'responses': ['Hello! how can i help you ?'],\n",
       "   'tag': 'Intro'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['thank you',\n",
       "    'thanks',\n",
       "    'cya',\n",
       "    'see you',\n",
       "    'later',\n",
       "    'see you later',\n",
       "    'goodbye',\n",
       "    'i am leaving',\n",
       "    'have a Good day',\n",
       "    'you helped me',\n",
       "    'thanks a lot',\n",
       "    'thanks a ton',\n",
       "    'you are the best',\n",
       "    'great help',\n",
       "    'too good',\n",
       "    'you are a good learning buddy'],\n",
       "   'responses': ['I hope I was able to assist you, Good Bye'],\n",
       "   'tag': 'Exit'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['olympus',\n",
       "    'explain me how olympus works',\n",
       "    'I am not able to understand olympus',\n",
       "    'olympus window not working',\n",
       "    'no access to olympus',\n",
       "    'unable to see link in olympus',\n",
       "    'no link visible on olympus',\n",
       "    'whom to contact for olympus',\n",
       "    'lot of problem with olympus',\n",
       "    'olypus is not a good tool',\n",
       "    'lot of problems with olympus',\n",
       "    'how to use olympus',\n",
       "    'teach me olympus'],\n",
       "   'responses': ['Link: Olympus wiki'],\n",
       "   'tag': 'Olympus'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['i am not able to understand svm',\n",
       "    'explain me how machine learning works',\n",
       "    'i am not able to understand naive bayes',\n",
       "    'i am not able to understand logistic regression',\n",
       "    'i am not able to understand ensemble techb=niques',\n",
       "    'i am not able to understand knn',\n",
       "    'i am not able to understand knn imputer',\n",
       "    'i am not able to understand cross validation',\n",
       "    'i am not able to understand boosting',\n",
       "    'i am not able to understand random forest',\n",
       "    'i am not able to understand ada boosting',\n",
       "    'i am not able to understand gradient boosting',\n",
       "    'machine learning',\n",
       "    'ML',\n",
       "    'SL',\n",
       "    'supervised learning',\n",
       "    'knn',\n",
       "    'logistic regression',\n",
       "    'regression',\n",
       "    'classification',\n",
       "    'naive bayes',\n",
       "    'nb',\n",
       "    'ensemble techniques',\n",
       "    'bagging',\n",
       "    'boosting',\n",
       "    'ada boosting',\n",
       "    'ada',\n",
       "    'gradient boosting',\n",
       "    'hyper parameters'],\n",
       "   'responses': ['Link: Machine Learning wiki '],\n",
       "   'tag': 'SL'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['what is deep learning',\n",
       "    'unable to understand deep learning',\n",
       "    'explain me how deep learning works',\n",
       "    'i am not able to understand deep learning',\n",
       "    'not able to understand neural nets',\n",
       "    'very diffult to understand neural nets',\n",
       "    'unable to understand neural nets',\n",
       "    'ann',\n",
       "    'artificial intelligence',\n",
       "    'artificial neural networks',\n",
       "    'weights',\n",
       "    'activation function',\n",
       "    'hidden layers',\n",
       "    'softmax',\n",
       "    'sigmoid',\n",
       "    'relu',\n",
       "    'otimizer',\n",
       "    'forward propagation',\n",
       "    'backward propagation',\n",
       "    'epochs',\n",
       "    'epoch',\n",
       "    'what is an epoch',\n",
       "    'adam',\n",
       "    'sgd'],\n",
       "   'responses': ['Link: Neural Nets wiki'],\n",
       "   'tag': 'NN'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['what is your name',\n",
       "    'who are you',\n",
       "    'name please',\n",
       "    'when are your hours of opertions',\n",
       "    'what are your working hours',\n",
       "    'hours of operation',\n",
       "    'working hours',\n",
       "    'hours'],\n",
       "   'responses': ['I am your virtual learning assistant'],\n",
       "   'tag': 'Bot'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['what the hell',\n",
       "    'bloody stupid bot',\n",
       "    'do you think you are very smart',\n",
       "    'screw you',\n",
       "    'i hate you',\n",
       "    'you are stupid',\n",
       "    'jerk',\n",
       "    'you are a joke',\n",
       "    'useless piece of shit'],\n",
       "   'responses': ['Please use respectful words'],\n",
       "   'tag': 'Profane'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['my problem is not solved',\n",
       "    'you did not help me',\n",
       "    'not a good solution',\n",
       "    'bad solution',\n",
       "    'not good solution',\n",
       "    'no help',\n",
       "    'wasted my time',\n",
       "    'useless bot',\n",
       "    'create a ticket'],\n",
       "   'responses': ['Tarnsferring the request to your PM'],\n",
       "   'tag': 'Ticket'}]}"
      ]
     },
     "execution_count": 249,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#import corpus\n",
    "with open('/gdrive/MyDrive/Colab Notebooks/Projects/NLP/GLBot.json') as file:\n",
    "  Corpus =json.load(file)\n",
    "\n",
    "#Display Corpus file\n",
    "Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzT54bGGGlu3"
   },
   "source": [
    "* Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the bag words the root word will be learnt in order to identify the correct tag and respond accoringly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ehlz-I2eGcIi",
    "outputId": "e7435440-8eca-42e3-f96e-248b157c0130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 313,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "id": "QNzPKD64HRk1"
   },
   "outputs": [],
   "source": [
    "W = [] # Tokens\n",
    "L = [] #identified tags\n",
    "doc_x = [] #Tokenised words\n",
    "doc_y = [] #Tags or labels\n",
    "for intent in Corpus['intents']:\n",
    "  for pattern in intent['patterns']:\n",
    "    w_temp = nltk.word_tokenize(pattern)\n",
    "    W.extend(w_temp)\n",
    "    doc_x.append(w_temp)\n",
    "    doc_y.append(intent['tag'])\n",
    "  if intent['tag'] not in L: \n",
    "    #add the missing tag if any\n",
    "    L.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "id": "4AiLX99RIiTQ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "#Stemming\n",
    "W = [stemmer.stem(w.lower()) for w in W if w != \"?\"] #learning the root word\n",
    "W = sorted(list(set(W))) # sorted words\n",
    "L = sorted(L) # sorted list of tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e5nsku4KFYW"
   },
   "source": [
    "* Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "npRtypGlJuTR"
   },
   "outputs": [],
   "source": [
    "Train = []\n",
    "Target = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(L))]\n",
    "\n",
    "#loop to create bow and put freq count on each word\n",
    "for x, doc in enumerate(doc_x):\n",
    "  bag = []\n",
    "\n",
    "  w_temp = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "  for w in W:\n",
    "    if w in w_temp:\n",
    "      bag.append(1)\n",
    "    else:\n",
    "      bag.append(0)\n",
    "  \n",
    "  output_row = out_empty[:]\n",
    "  output_row[L.index(doc_y[x])] = 1\n",
    "\n",
    "  Train.append(bag) #list\n",
    "  Target.append(output_row) #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "12lp9H1KLWI3"
   },
   "outputs": [],
   "source": [
    "#converting to np arrays for mdel to understand\n",
    "Train = np.array(Train)\n",
    "Target = np.array(Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKyz_9VKMIo8"
   },
   "source": [
    "* Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y86UvLEaMsC3",
    "outputId": "2958a7fb-1b31-48eb-a994-5938d9291066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
      "\r",
      "\u001b[K     |                             | 10kB 18.6MB/s eta 0:00:01\r",
      "\u001b[K     |                          | 20kB 15.7MB/s eta 0:00:01\r",
      "\u001b[K     |                      | 30kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |                   | 40kB 12.8MB/s eta 0:00:01\r",
      "\u001b[K     |                | 51kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |             | 61kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |          | 71kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |       | 81kB 9.2MB/s eta 0:00:01\r",
      "\u001b[K     |    | 92kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     | | 102kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     || 112kB 8.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tflearn: filename=tflearn-0.5.0-cp37-none-any.whl size=127301 sha256=472b3c86594a23422dbf24a9108d23400b521e2b64b6f1ad96466a923dd7f318\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install tflearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4SAhlmaMG7X",
    "outputId": "35731dc6-6244-435c-9b42-d2db73297be0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "\n",
    "NN = tflearn.input_data(shape=[None, len(Train[0])]) #input layer\n",
    "NN = tflearn.fully_connected(NN,8) #dense layer with 8 neuron\n",
    "NN = tflearn.fully_connected(NN,8) #dense layer with 8 neuron\n",
    "NN = tflearn.fully_connected(NN,len(Target[0]),activation=\"softmax\") #output layer\n",
    "NN = tflearn.regression(NN)\n",
    "\n",
    "model = tflearn.DNN(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioOaVUCfOSLL"
   },
   "source": [
    "* Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3R8pHCroNyBs",
    "outputId": "ba093358-41e6-4365-805c-1f91806cc2d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3769  | total loss: \u001b[1m\u001b[32m0.01900\u001b[0m\u001b[0m | time: 0.064s\n",
      "| Adam | epoch: 236 | loss: 0.01900 - acc: 1.0000 -- iter: 120/128\n",
      "Training Step: 3770  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.068s\n",
      "| Adam | epoch: 236 | loss: 0.02066 - acc: 1.0000 -- iter: 128/128\n",
      "--\n",
      "INFO:tensorflow:/gdrive/My Drive/Colab Notebooks/Projects/NLP/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.fit(Train, Target, n_epoch = 200, batch_size =8, show_metric = True)\n",
    "model.save(\"model.tflearn\") # save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1v7_ySTORmX"
   },
   "source": [
    "* Design Interactive utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "id": "8CjNvUTeO-xm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def bag_of_words(s, W):\n",
    "  bag = [0 for _ in range(len(W))]\n",
    "\n",
    "  s_words = nltk.word_tokenize(s)\n",
    "  s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "  for se in s_words:\n",
    "    for i,w in enumerate(W):\n",
    "      if w == se:\n",
    "        bag[i] = 1\n",
    "\n",
    "  return np.array(bag)\n",
    "\n",
    "def chat():\n",
    "  print(\"GLBOT: My name is GLbot. I will answer your queries about Great learning If you want to exit, type Bye!\")\n",
    "  print(\"GLBOT: If answer is not right (type:*)\")\n",
    "  while(flag==True):\n",
    "    inp = input(\"\\n\\nYou: \")\n",
    "    inp = inp.lower()\n",
    "    if inp == \"*\":\n",
    "      print(\"GLBOT: Please rephrase your query amd try again\")\n",
    "    if inp == \"bye\":\n",
    "      print(\"GLBOT: Bye! take care..\")\n",
    "      break\n",
    "    \n",
    "    results = model.predict([bag_of_words(inp,W)])\n",
    "    results_index = np.argmax(results)\n",
    "    tag = L[results_index]\n",
    "    \n",
    "    for tg in Corpus['intents']:\n",
    "      if tg['tag'] == tag:\n",
    "        #print(tg['responses'])\n",
    "        responses = tg['responses']\n",
    "    \n",
    "    if tag in ['SL','NN', 'Olympus']:\n",
    "      print('GLBOT: Please use the ', random.choice(responses))\n",
    "    else:\n",
    "      print('GLBOT: ', random.choice(responses))\n",
    "  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO3D5udoSOTE",
    "outputId": "5e135a00-a7c2-4dc4-a944-8a15047dd9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLBOT: My name is GLbot. I will answer your queries about Great learning If you want to exit, type Bye!\n",
      "GLBOT: If answer is not right (type:*)\n",
      "\n",
      "\n",
      "You: Hi\n",
      "GLBOT:  Hello! how can i help you ?\n",
      "\n",
      "\n",
      "You: who r you\n",
      "GLBOT:  I am your virtual learning assistant\n",
      "\n",
      "\n",
      "You: whats ur name\n",
      "GLBOT:  I am your virtual learning assistant\n",
      "\n",
      "\n",
      "You: i dont have access to olympus\n",
      "GLBOT:  Tarnsferring the request to your PM\n",
      "\n",
      "\n",
      "You: i do not understand deep learning\n",
      "GLBOT: Please use the  Link: Neural Nets wiki\n",
      "\n",
      "\n",
      "You: need help on supervised learning\n",
      "GLBOT: Please use the  Link: Machine Learning wiki \n",
      "\n",
      "\n",
      "You: you are not worth\n",
      "GLBOT:  Hello! how can i help you ?\n",
      "\n",
      "\n",
      "You: hell\n",
      "GLBOT:  Hello! how can i help you ?\n",
      "\n",
      "\n",
      "You: what the hell\n",
      "GLBOT:  Please use respectful words\n",
      "\n",
      "\n",
      "You: thank you\n",
      "GLBOT:  I hope I was able to assist you, Good Bye\n",
      "\n",
      "\n",
      "You: bye\n",
      "GLBOT: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIPqg6y6trH9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqGe2jlRinhe"
   },
   "source": [
    "### ****************** THANK YOU *******************"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "R8_NLP_Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
